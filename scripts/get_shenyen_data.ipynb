{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import json\n",
    "import time\n",
    "import math \n",
    "import httplib2\n",
    "import requests\n",
    "import pinecone \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from youtubesearchpython import *\n",
    "from langchain.llms import OpenAIChat\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import VectorDBQAWithSourcesChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shenyen GPT\n",
    "\n",
    "- Scrape source from: https://miblue119.github.io/shengyen_lectures/index.html\n",
    "- Chunk data \n",
    "- Embed it to Pineconde\n",
    "- Test VectorDBQA chain on it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. Scrape transcriptions -` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the code based on Lex-GPT's script\n",
    "# Get text -\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def get_text_and_title(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    t=(text_from_html(html))\n",
    "    title=t.split(\"|\")[0].split(\"back to index\")[1].strip(\"link \")\n",
    "    return t, title\n",
    "\n",
    "# Get links -\n",
    "def get_links(URL):\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(URL)\n",
    "    links = []\n",
    "    for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "        if link.has_attr('href'):\n",
    "            links.append(link['href'])\n",
    "    links_clean = [l for l in links if \"https\" in l]\n",
    "    return links_clean\n",
    "\n",
    "# Get image -\n",
    "def get_img(URL,title,episode_id):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    urls = [img['src'] for img in img_tags]\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        imgpath=\"../public/0%s.jpg\"%episode_id\n",
    "        with open(imgpath, 'wb') as f:\n",
    "            if 'http' not in url:\n",
    "                url = '{}{}'.format(site, url)\n",
    "            response = requests.get(url)\n",
    "            f.write(response.content)\n",
    "    return imgpath\n",
    "\n",
    "# Full pipeline - \n",
    "def pre_process(URL,episode_id):\n",
    "\n",
    "    t,title=get_text_and_title(URL)\n",
    "    # print(f\"Title: {title}\")\n",
    "    # print(f\"Episode ID: {episode_id}\")\n",
    "    links=get_links(URL)\n",
    "    # print(f\"Links: {len(links)}\")\n",
    "    img=get_img(URL,title,episode_id)\n",
    "    stor_chunk = pd.DataFrame()\n",
    "    stor_chunk['chunks']= t.split(\"link |\")\n",
    "    # print(f\"stor_chunk['chunks']: {stor_chunk['chunks']}\")\n",
    "    #stor_chunk['clean_chunks']=stor_chunk['chunks'].apply(lambda x: re.sub(r\"[^a-zA-Z ]+\", '', x)).apply(lambda x: x.strip())\n",
    "    stor_chunk['clean_chunks'] = stor_chunk['chunks'].apply(lambda x: re.sub(r\"[^a-zA-Z\\u4e00-\\u9fff ]+\", '', x)).apply(lambda x: x.strip())\n",
    "    # print(f\"stor_chunk['clean_chunks']: {stor_chunk['clean_chunks']}\")\n",
    "    stor_chunk['links']=links\n",
    "    # print(f\"stor_chunk['links']: {stor_chunk['links']}\")\n",
    "    all_text = stor_chunk['clean_chunks'].str.cat(sep=' ')\n",
    "    # print(f\"all_text: {all_text}\")\n",
    "    return all_text, links, title\n",
    "\n",
    "# Make splits - \n",
    "def make_splits(chunks, overlap_scale, URL):\n",
    "\n",
    "    # ID\n",
    "    # 'https://miblue119.github.io/shengyen_lectures/0001/0000/0000.html'\n",
    "    # Get the last 3 parts of the URL and generate id to 0001_0000_0000\n",
    "    episode_id='_'.join(URL.strip('.html').split('/')[-3:])\n",
    "\n",
    "    # Pre-processing\n",
    "    texts,links,title=pre_process(URL,episode_id)\n",
    "    print(texts)\n",
    "    \n",
    "    # Splits \n",
    "    chunk_overlap = int(chunks * overlap_scale)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, \n",
    "                                                   chunk_overlap=chunk_overlap) \n",
    "    texts_recusive = text_splitter.split_text(texts)\n",
    "    print(len(texts_recusive)) \n",
    "\n",
    "    # Metadata \n",
    "    N = len(texts_recusive) \n",
    "    bins = np.linspace(0, len(links)-1, N, dtype=int)\n",
    "    sampled_links = [links[i] for i in bins]\n",
    "    # Here we can add \"link\", \"title\", etc that can be fetched in the app \n",
    "    metadatas=[{\"source\":title + \" \" +link,\"id\":episode_id,\"link\":link,\"title\":title} for link in sampled_links]\n",
    "    print(len(metadatas))\n",
    "    return texts_recusive,metadatas,title,episode_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all pages \n",
    "http = httplib2.Http()\n",
    "status, response = http.request(\"https://miblue119.github.io/shengyen_lectures/index.html\")\n",
    "links = []\n",
    "for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "    if link.has_attr('href'):\n",
    "        links.append(link['href'])\n",
    "links_tx = [\"https://miblue119.github.io/shengyen_lectures/\"+l for l in links if \"0\" in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtitiles_page_links(index_page_url):\n",
    "    # Retrieve the lecture index page\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(index_page_url)\n",
    "    links = []\n",
    "    for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "        if link.has_attr('href'):\n",
    "            links.append(link['href'])\n",
    "    lectures_base_url = index_page_url.split(\"index.html\")[0] \n",
    "    links_lectures = [lectures_base_url+l for l in links if \"0\" in l]\n",
    "    # Remove the link ending without `.html`\n",
    "    links_lectures = [l for l in links_lectures if l.endswith('.html')]\n",
    "    # Retrieve the lecture subtitiles page\n",
    "    links_subtitles = []\n",
    "    for link_lecture in links_lectures:\n",
    "        #print(link_lecture)\n",
    "        status, response = http.request(link_lecture)\n",
    "        subtile_links_candidates = []\n",
    "        for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "            if link.has_attr('href'):\n",
    "                subtile_links_candidates.append(link['href'])\n",
    "        subtile_links = [l for l in subtile_links_candidates if \"0\" in l]\n",
    "        # Remove the link ending without `.html`\n",
    "        subtile_links = [l for l in subtile_links if l.endswith('.html')]\n",
    "        \n",
    "        prefix_link_lecture = \"/\".join(link_lecture.split(\"/\")[:-1])\n",
    "        subtile_links = [prefix_link_lecture + \"/\" + l for l in subtile_links]\n",
    "        links_subtitles.extend(subtile_links)\n",
    "    return links_subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_subtitles = get_subtitiles_page_links(\"https://miblue119.github.io/shengyen_lectures/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Chunk size: key parameter *** \n",
    "import os \n",
    "chunks = 200\n",
    "overlap_scale = 0.2\n",
    "splits_scrape = [ ]\n",
    "metadatas_scrape = [ ]\n",
    " \n",
    "transcripts_dir = \"docs\"\n",
    "metadatas_dir = \"metadatas\"\n",
    "os.makedirs(transcripts_dir, exist_ok=True)\n",
    "os.makedirs(metadatas_dir, exist_ok=True)\n",
    "\n",
    "# Iterate \n",
    "stor=pd.DataFrame()\n",
    "for page in links_subtitles:\n",
    "    try:\n",
    "        print(\"Writing: %s\"%page)\n",
    "        # Make splits\n",
    "        splits,metadatas,title,episode_id=make_splits(chunks,overlap_scale, page)\n",
    "        stor.loc[episode_id,'title']=title \n",
    "        with open(f'{transcripts_dir}/%s.txt'%episode_id, \"w\") as f:\n",
    "            for string in splits:\n",
    "                f.write(string + \"\\n\") \n",
    "        f.close()\n",
    "        with open(f'{metadatas_dir}/%s.json'%episode_id, \"w\") as f:\n",
    "            # Dump the json with indent=4\n",
    "            json.dump(metadatas, f, indent=4)\n",
    "        f.close()\n",
    "        splits_scrape.append(splits)\n",
    "        metadatas_scrape.append(metadatas)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error on page: %s\"%page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits_scrape), len(metadatas_scrape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. Embed full dataset in Pinecone VectorDB -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the list of lists \n",
    "splits_all = []\n",
    "# For the initial write \n",
    "# for sublist in [splits_scrape+splits_new]:\n",
    "# For updates -- \n",
    "for sublist in splits_scrape:\n",
    "    splits_all.extend(sublist)\n",
    "\n",
    "metadatas_all = []\n",
    "# For the initial write \n",
    "# for sublist in [metadatas_scrape+metadatas_new]:\n",
    "# For updates -- \n",
    "for sublist in metadatas_scrape:\n",
    "    metadatas_all.extend(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits_all), len(metadatas_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "index_name = \"shenyen-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize with small set of data - \n",
    "p = Pinecone.from_texts(splits_all[0:2], \n",
    "                           embeddings, \n",
    "                           index_name=index_name, \n",
    "                           metadatas=metadatas_all[0:2])\n",
    "\n",
    "# Update - \n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data in chunk to avoid data ingest errors\n",
    "chunk_size = 100\n",
    "last_chunk = 0\n",
    "num_chunks = math.ceil(len(splits_all) / chunk_size)\n",
    "for i in range(last_chunk,num_chunks):\n",
    "    \n",
    "    print(i)\n",
    "    start_time = time.time()\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, len(splits_all))\n",
    "    \n",
    "    # Extract the current chunk\n",
    "    current_splits = splits_all[start_idx:end_idx]\n",
    "    current_metadatas = metadatas_all[start_idx:end_idx]\n",
    "    \n",
    "    # Add the current chunk to the vector database\n",
    "    p.add_texts(texts = current_splits, metadatas=current_metadatas)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. Read in VectorDB for testing` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "index_name = \"shenyen-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4. Run VectorDBQAWithSourcesChain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vectordb_sources_chain(llm,query,docstore):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    chain = VectorDBQAWithSourcesChain.from_chain_type(llm, chain_type=\"stuff\", vectorstore=docstore)\n",
    "    a = chain({\"question\": query},return_only_outputs=True)\n",
    "    print(a[\"answer\"])\n",
    "    print(a[\"sources\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "llm = OpenAIChat(temperature=0)\n",
    "q = \"學佛可以幹嘛?#lang:zh-Hant\"\n",
    "run_vectordb_sources_chain(llm,q,p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
